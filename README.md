# Structured Questionnaire Answering Tool

## Overview

This project is a secure, API-driven application that automates structured questionnaire answering using internal reference documentation.

It simulates how companies respond to security reviews, compliance audits, vendor assessments, and operational questionnaires in a structured and grounded manner.

The system:

- Authenticates users
- Stores questionnaires and reference documents
- Retrieves relevant internal documentation
- Generates grounded answers with citations
- Provides confidence scores and a coverage summary
- Exports a structured document preserving the original format


## Industry and Fictional Company

**Industry:** Cybersecurity SaaS

**Fictional Company:**

ShieldOps AI is a fictional cybersecurity SaaS company providing cloud-native threat detection, compliance automation, and security posture monitoring for enterprise clients operating in regulated environments.

The system uses internal documents such as:

- Security policy
- Compliance reports
- Backup policies
- Infrastructure documentation
- Incident response procedures

These act as the "source of truth" for answering questionnaires.


## Core Features

### 1. User Authentication

- JWT-based authentication
- Secure signup and login
- Password hashing using industry-standard libraries
- Protected API endpoints

### 2. Questionnaire Upload and Parsing

- Upload text-based questionnaire files
- Automatically parses each non-empty line into an individual question
- Stores questions in a relational database
- Maintains linkage between user and uploaded content

### 3. Reference Document Management

- Upload multiple internal reference documents
- Documents are stored per user
- Used as the source material for answer generation

### 4. AI-Based Answer Generation

- Implements a retrieval-based workflow
- Matches questions against reference documents using keyword-based logic
- Generates structured answers grounded in retrieved content
- Attaches at least one citation (source filename) per answer
- Returns "Not found in references." if no supporting content is identified

### 5. Confidence Scoring

- Each generated answer includes a confidence level (High, Medium, or Low)
- Confidence is based on retrieval match strength

### 6. Coverage Summary

Each generation run provides:

- Total number of questions
- Number of questions answered with supporting references
- Number of questions marked as "Not found in references."

### 7. Structured Export

- Generates a downloadable `.docx` file
- Preserves original question structure and order
- Inserts answers directly below each question
- Includes citations for each answer


## System Architecture

### Backend
- FastAPI

### Database
- SQLite
- SQLAlchemy ORM

### Authentication
- JWT (JSON Web Tokens)
- Passlib for password hashing

### Retrieval Logic
- Keyword-based retrieval (mock RAG implementation)
- Citation metadata stored alongside documents

### Document Export
- python-docx for generating structured output documents

### Interface
- Swagger UI (auto-generated by FastAPI) used for interacting with the system


## Workflow

1. User signs up and logs in
2. User uploads a questionnaire document
3. User uploads reference documents
4. User triggers answer generation
5. System retrieves relevant document content and generates grounded answers
6. User reviews answers along with citations and confidence scores
7. User exports a structured document


## Assumptions

- Questionnaire files are plain text documents
- Each non-empty line represents a separate question
- Reference documents are plain text files
- Retrieval is keyword-based for demonstration purposes
- Each user operates independently with their own data

## Project Structure

structured-qa-tool/

backend/
- main.py
- models.py
- database.py
- auth.py
- rag.py
- utils.py
- requirements.txt

mock_data/
- questionnaire.txt
- security_policy.txt
- compliance_report.txt
- backup_policy.txt
- incident_response.txt
- infrastructure_overview.txt

README.md
.gitignore


## How to Run

1. Navigate to the backend directory

2. Create a virtual environment:
   python -m venv venv

3. Activate the environment:

   Windows:
   venv\Scripts\activate

   macOS/Linux:
   source venv/bin/activate

4. Install dependencies:
   pip install -r requirements.txt

5. Start the server:
   python -m uvicorn main:app --reload

6. Open in browser:
   http://127.0.0.1:8000/docs


