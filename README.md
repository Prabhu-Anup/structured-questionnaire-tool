Structured Questionnaire Answering Tool
ğŸš€ Overview

This project is a secure, AI-powered web application that automates structured questionnaire answering using internal reference documentation.

It simulates how companies respond to security reviews, compliance audits, vendor assessments, and operational questionnaires in a structured and grounded way.

The system:

Authenticates users

Stores questionnaires and reference documents

Retrieves relevant internal documentation

Generates grounded answers with citations

Provides confidence scores and coverage summary

Exports a structured document preserving original format

ğŸ¢ Industry & Fictional Company
Industry

Cybersecurity SaaS

Fictional Company

ShieldOps AI is a fictional cybersecurity SaaS company providing cloud-native threat detection, compliance automation, and security posture monitoring for enterprise clients operating in regulated environments.

The system uses internal documents such as:

Security policy

Compliance reports

Backup policies

Infrastructure documentation

Incident response procedures

These act as the "source of truth" for answering questionnaires.

ğŸ§  What This System Does
1ï¸âƒ£ User Authentication

JWT-based authentication

Secure signup and login

Protected endpoints

2ï¸âƒ£ Questionnaire Upload

Upload text-based questionnaire

Automatically parses into individual questions

Stores questions in database

3ï¸âƒ£ Reference Document Upload

Upload multiple internal documents

Stored per-user

Acts as ground truth for answer generation

4ï¸âƒ£ AI-Powered Answer Generation

Retrieves relevant content using keyword-based mock RAG

Generates structured answers

Attaches citation (source filename)

Returns "Not found in references." if unsupported

5ï¸âƒ£ Confidence Score

Each answer includes a confidence level:

High

Medium

Low

Based on retrieval match strength.

6ï¸âƒ£ Coverage Summary

Each run provides a summary:

Total questions

Questions answered

Questions not found

7ï¸âƒ£ Export Functionality (Phase 2)

Preserves original questionnaire structure

Keeps questions unchanged

Inserts answers below each question

Includes citations

Downloads structured .docx file

ğŸ— Architecture
Backend

FastAPI

Database

SQLite

SQLAlchemy ORM

Authentication

JWT (JSON Web Tokens)

Password hashing using Passlib

Retrieval Logic

Keyword-based matching (mock RAG)

Citation metadata stored with documents

Document Export

python-docx

ğŸ”„ User Workflow

User signs up / logs in

Uploads questionnaire

Uploads reference documents

Clicks Generate Answers

Reviews answers with citations & confidence

Exports structured document

This ensures a complete workflow from upload â†’ review â†’ export.

ğŸ“Š API Response Structure
Generate Answers Response
{
  "summary": {
    "total_questions": 10,
    "answered": 9,
    "not_found": 1
  },
  "results": [
    {
      "question": "...",
      "answer": "...",
      "citation": "security_policy.txt",
      "confidence": "High"
    }
  ]
}
ğŸ“Œ Assumptions

Questionnaire is plain text (each line = one question)

Reference documents are plain text files

Retrieval is keyword-based (mock AI stage)

Each user operates on their own dataset

Only latest questionnaire is processed for generation

âš– Trade-offs

Used keyword-based retrieval instead of embeddings for simplicity

SQLite used instead of production-grade PostgreSQL

No frontend UI (Swagger used for demonstration)

No chunk-level retrieval segmentation

Answers are snippet-based rather than summarized

These trade-offs were made to ensure end-to-end completeness within scope.

ğŸ”® What I Would Improve With More Time

If extended further, I would:

Replace keyword retrieval with embedding-based semantic search

Use a vector database (e.g., Chroma or Pinecone)

Implement document chunking

Add partial regeneration per question

Add answer editing UI

Add version history tracking

Add async background processing

Deploy on cloud (Render / AWS)

Implement citation snippet highlighting

Add evaluation metrics for retrieval quality

ğŸ” Grounding & Reliability

The system ensures reliability by:

Retrieving answers strictly from reference documents

Attaching explicit citations

Returning "Not found in references." when unsupported

Providing confidence scores

Preventing hallucinated content

This ensures structured, explainable AI outputs.

ğŸ“‚ Project Structure
structured-qa-tool/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ database.py
â”‚   â”œâ”€â”€ auth.py
â”‚   â”œâ”€â”€ rag.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ app.db
â”‚   â””â”€â”€ venv/
â”‚
â”œâ”€â”€ mock_data/
â”‚   â”œâ”€â”€ questionnaire.txt
â”‚   â”œâ”€â”€ security_policy.txt
â”‚   â”œâ”€â”€ compliance_report.txt
â”‚   â”œâ”€â”€ backup_policy.txt
â”‚   â”œâ”€â”€ incident_response.txt
â”‚   â””â”€â”€ infrastructure_overview.txt
â”‚
â””â”€â”€ README.md
â–¶ How To Run

Create virtual environment

Install dependencies

pip install fastapi uvicorn sqlalchemy passlib python-jose python-multipart python-docx

Run server

python -m uvicorn main:app --reload

Open Swagger UI

http://127.0.0.1:8000/docs
âœ… Assignment Criteria Coverage

âœ” User authentication
âœ” Persistent storage
âœ” Structured upload â†’ generation â†’ export flow
âœ” AI-based retrieval logic
âœ” Citation grounding
âœ” "Not found in references" logic
âœ” Review before export
âœ” Document export preserving structure
âœ” Two Nice-to-Have features implemented

ğŸ§¾ Final Note

This project demonstrates practical system design thinking for building grounded AI workflows in real-world enterprise scenarios involving structured documentation and compliance processes.

It prioritizes:

Reliability

Explainability

Clear workflow

Structured outputs

Trade-off awareness